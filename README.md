#  Automated Requirements Labeling Tool

Questo progetto fornisce una pipeline di scripting in **Python** per **preâ€‘elaborare**, **etichettare** e **selezionare** automaticamente requisiti in linguaggio naturale a partire da un dataset testuale e da dizionari di categorie.  
La pipeline produce dapprima un dataset etichettato (CSV) e quindi, tramite passi postâ€‘processing, **suddivide per categoria** e **seleziona un sottoinsieme di requisiti** per le analisi successive.

---

##  Features

- **Preâ€‘elaborazione automatica** del dataset `.arff` (pulizia intestazioni/commenti).  
- **ID univoci** dei requisiti (`R1`, `R2`, â€¦) per tracciabilitÃ .  
- **Etichettatura basata su dizionari** (`NewDict/*.txt`, una categoria per file).  
- **Matching contestuale (spaCy)** con POS tagging per disambiguare termini polisemici.  
- **Supporto frasi multiâ€‘parola (FlashText)** ad alte prestazioni.  
- **Postâ€‘processing completo** dopo lâ€™etichettatura:
  - **Splitter**: crea 19 file (uno per categoria) con i requisiti etichettati.
  - **Selecter**: seleziona **N requisiti casuali** per categoria (default 27) e li consolida in un unico CSV finale.
- **Utility per dizionari**:
  - **MergeDict**: unisce due file di termini â€œvaghiâ€ e genera statistiche di overlap.

---

##  Struttura del Progetto

```bash
/progetto/
â”‚
â”œâ”€â”€ NewDict/                     # dizionari (una categoria per file .txt)
â”‚   â”œâ”€â”€ noun.txt
â”‚   â”œâ”€â”€ verb.txt
â”‚   â”œâ”€â”€ adj.txt
â”‚   â””â”€â”€ vague.txt                 # (generato da MergeDict)
â”‚   â””â”€â”€ ...                       # altre categorie
â”‚
â”œâ”€â”€ Dataset.arff                 # dataset iniziale dei requisiti
â”œâ”€â”€ AssociazioneID.py            # step 1: assegna ID e normalizza
â”œâ”€â”€ tool.py                      # step 2: etichetta usando NewDict + spaCy + FlashText
â”œâ”€â”€ Splitter.py                  # step 3: split per categoria (post-tool.py)
â”œâ”€â”€ Selecter.py                  # step 4: selezione casuale per categoria (post-split)
â”œâ”€â”€ MergeDict.py                 # utility: merge di due dizionari â€œvaghiâ€
â”‚
â”œâ”€â”€ Dataset_With_R_ID.txt        # (generato)
â”œâ”€â”€ Labeled_Dataset.csv          # (generato)
â”œâ”€â”€ Sorted_by_Categories/        # (generato da Splitter.py)
â”‚   â”œâ”€â”€ <categoria>_requirements.csv
â”‚   â””â”€â”€ ...
â”‚â”€â”€ Vague_1.txt
â”‚â”€â”€ Vagues_2.txt
â”‚
â””â”€â”€ README.md
```

---

##  Requisiti e Installazione

- **Python** 3.7+
- Dipendenze:
```bash
pip install spacy flashtext
python -m spacy download en_core_web_sm
```

---

##  Workflow Endâ€‘toâ€‘End

Di seguito il flusso operativo **completo**, con i passi â€œpostâ€‘tool.pyâ€ **integrati** perchÃ© necessari alla fase di **selezione** dei requisiti per lâ€™analisi.

### 1) Preâ€‘elaborazione & ID â€” `AssociazioneID.py`
Legge `Dataset.arff`, rimuove intestazioni/commenti e assegna un ID univoco a ogni requisito.

```bash
python AssociazioneID.py
```
**Output**: `Dataset_With_R_ID.txt`  
Esempio riga:
```
R1: 1,'The system shall refresh the display every 60 seconds.',PE
```

### 2) Etichettatura â€” `tool.py`
Analizza ed etichetta ogni requisito utilizzando i dizionari in `NewDict/`, spaCy (POS) e FlashText (frasi).

```bash
python tool.py
```
**Output**: `Labeled_Dataset.csv` (delimitatore `;`)  
Colonne: `ID;ID progetto;REQUISITO (testo);Classe dei requisiti;CATEGORIA;PAROLA`

> Se un requisito ha piÃ¹ match, genera piÃ¹ righe. Se non ha match, `CATEGORIA` e `PAROLA` sono `NULL`.

### 3) Split per categoria â€” `Splitter.py` ï¸  (POSTâ€‘tool.py)
Legge `Labeled_Dataset.csv` e crea **19 file CSV**, uno per ciascuna categoria del dizionario, dentro `Sorted_by_Categories/`.

```bash
python Splitter.py
```
**Output**: `Sorted_by_Categories/<categoria>_requirements.csv` (con intestazione).  
Ogni file contiene **tutti i requisiti** etichettati con quella categoria.

### 4) Selezione campione â€” `Selecter.py` ï¸  (POSTâ€‘split)
Per ogni file in `Sorted_by_Categories/`, seleziona **N requisiti casuali** (default **27**, modificabile nello script).  
Se il file contiene meno di N requisiti, li include **tutti**. Infine consolida tutto in un unico CSV.

```bash
python Selecter.py
```
**Output**: `Requisiti_Selezionati.csv`  
**RandomicitÃ **: la selezione Ã¨ **casuale** â†’ **non Ã¨ garantito** ottenere lo stesso output finale a esecuzioni differenti.  
Per riproducibilitÃ , imposta un seed allâ€™inizio dello script (es. `random.seed(42)`).

---

##  Logica di Etichettatura (`tool.py`)

### Caricamento dizionari (`load_all_dicts_optimized`)
- **Parole singole** â†’ normalizzate e mappate in `singles_category_map` (parola â†’ set categorie).  
- **Frasi multiâ€‘parola** â†’ caricate in `KeywordProcessor` (FlashText) per matching veloce e scalabile.

### Tokenizzazione & Matching (`tokenize_and_match_with_spacy`)
1. **Frasi multiâ€‘parola**: prioritarie (FlashText, con `span_info=True` per recupero esatto).  
2. **Token singoli**: analizzati con **spaCy** (lemma, POS).  
3. **Disambiguazione**: il lemma del token viene confrontato con `singles_category_map` e validato tramite `POS_CATEGORY_MAPPING` (es. `noun` â†’ {`NOUN`,`PROPN`}).

**Esempio**  
Testo: â€œThe system must display a warning message.â€  
Se `display` Ã¨ in `noun.txt` e `verb.txt`, spaCy lo marca **VERB** â†’ categoria finale `verb` (riduzione falsi positivi).

---

##  Utility Dizionari â€” `MergeDict.py` (opzionale ma utile)

**Scopo**: unire due elenchi di termini â€œvaghiâ€ in un unico file senza duplicati e generare statistiche di overlap. Utile per **curare/aggiornare** i dizionari prima di lanciare la pipeline.

**Input**: `Vague_1.txt`, `Vagues_2.txt`  
**Output**:
- `vague.txt` â€” unione **ordinata** di tutte le parole uniche dei due file.  
- `statistiche_file_uniti.txt` â€” contiene:
  - conteggio parole **solo** nel primo file;
  - conteggio parole **solo** nel secondo;
  - conteggio ed **elenco** parole **comuni**;
  - percentuali riepilogative.

**Esecuzione**:
```bash
python MergeDict.py
```

---

## ğŸ› ï¸ Personalizzazione Rapida

- **Percorsi file**: modifica `DICTIONARIES_DIR`, `REQUIREMENTS_FILE`, `OUTPUT_FILE` in `tool.py`.  
- **Nuove categorie**: aggiungi un file `.txt` in `NewDict/` e aggiorna `POS_CATEGORY_MAPPING` (mappa categoria â†’ POS spaCy).  
- **Parsing input**: la regex `REQUIREMENT_LINE_PARSE_RX` in `tool.py` definisce il formato di parsing delle righe di `Dataset_With_R_ID.txt`.  
- **Campione Selecter**: in `Selecter.py` cambia il numero N di requisiti da selezionare (default 27). Per risultati ripetibili, imposta un seed casuale.

---

##  Quick Start (tutto in fila)

```bash
# 1) Pre-elaborazione & ID
python AssociazioneID.py

# 2) Etichettatura
python tool.py

# 3) Split per categoria
python Splitter.py

# 4) Selezione campione (random)
python Selecter.py
```

Output principali:
- `Labeled_Dataset.csv` â†’ dataset etichettato completo  
- `Sorted_by_Categories/` â†’ 19 file, uno per categoria  
- `Requisiti_Selezionati.csv` â†’ campione finale per lâ€™analisi

---

## Autore

Progetto per la **qualitÃ  e lâ€™analisi dei requisiti** tramite NLP, dizionari semantici e pipeline di postâ€‘processing (split & selezione).



